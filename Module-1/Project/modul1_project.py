# -*- coding: utf-8 -*-
"""Modul1.project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QjHlNhyoAaXVUTfkCoBvvHTQm2uXh7tT
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files

uploaded = files.upload()

df = pd.read_csv('Coffe_sales.csv')
df.head()

# rename columns
df.rename(columns = {'hour_of_day': 'hour',
                     'cash_type': 'payment_method',
                     'money': 'amount',
                     'coffee_name': 'product_name',
                     'Time_of_Day': 'time_of_day',
                     'Weekday': 'day',
                     'Month_name': 'month',
                     'Weekdaysort': 'weekday_num',
                     'Monthsort': 'month_num',
                     'Date': 'date',
                     'Time': 'time',
                     }, inplace = True)

df.columns

df.info()

df.describe(include='all')

df.duplicated().any()

df.isnull().sum()

df.shape

df.head()

import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency

# Put the days in the correct order
day_order = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']
if 'day' in df.columns:
    df['day'] = pd.Categorical(df['day'], categories=day_order, ordered=True)

# product and Day frequency table
ct = pd.crosstab(df['product_name'], df['day'])

# Chi-square testi
chi2, p, dof, expected = chi2_contingency(ct, correction=False)

# Cramer's V
n = ct.values.sum()
k = min(ct.shape) - 1
cramers_v = np.sqrt(chi2 / (n * k))

print(f"Chi2 = {chi2:.2f}, df = {dof}, p-value = {p:.4g}, Cramér's V = {cramers_v:.3f}")

# Post-hoc: adjusted residuals
expected_df = pd.DataFrame(expected, index=ct.index, columns=ct.columns)
row_tot = ct.sum(axis=1).values.reshape(-1,1)
col_tot = ct.sum(axis=0).values.reshape(1,-1)

adj_resid = (ct - expected_df) / np.sqrt(
    expected_df * (1 - row_tot/n) * (1 - col_tot/n)
)
# |adj_resid| >= 2
sig_cells = (adj_resid.abs() >= 2)

# summary: Which product standart out on which day?
hotspots = []
for i, prod in enumerate(ct.index):
    for j, d in enumerate(ct.columns):
        if sig_cells.iloc[i, j]:
            hotspots.append((prod, d, adj_resid.iloc[i, j]))
hotspots[:10]  # first 10

"""#EDA(Exploratory Data Analysis)

"""

# 1000 select random index
random_idx = np.random.choice(df.index, size=1000, replace=False)

# doing cash
df.loc[random_idx,"payment_method"] = "cash"

# control
print(df["payment_method"].value_counts())

df.groupby('payment_method')['amount'].sum()

df.groupby('payment_method').size().plot(kind='pie', autopct='%1.1f%%' ,ylabel="")
plt.title("Payment Method Distribution")
plt.show()

df.groupby('product_name')['amount'].sum()

print(df.columns.tolist())

# 1.Best Selling Coffee Chart
fig, ax = plt.subplots(figsize=(8, 4))
sns.barplot(x=df["product_name"].value_counts().index,y=df["product_name"].value_counts().values,palette="viridis")
plt.xlabel("Product Name")
plt.ylabel("Number of Sales")
plt.title("Best-selling product") # customer preference
plt.tight_layout()
plt.show()

df['product_name'].value_counts()

df.groupby('product_name')['amount'].sum().sort_values(ascending=False)

#2.saatlik satis yogunlugu
sns.lineplot(x="hour", y="amount", data=df)
plt.title("Hourly Sales Trend")
plt.xlabel("Hour of Day")
plt.ylabel("Average Sales Amount")
plt.tight_layout()
plt.show()

df["datetime"] = pd.to_datetime(df["date"] + " " + df["time"], format="%Y-%m-%d %H:%M:%S", errors="coerce")
df

#aylara gore gunluk satis trendi
daily_sales = df.groupby(df["datetime"].dt.date)["amount"].sum()
daily_sales.plot(kind="line", figsize=(10,4), title="Daily Sales Trend")

"""The plot shows the change in daily/monthly sales over time. Sales are high in June, drop in July and especially in August, then recover strongly in September and rise again by November. These changes may be due to missing data, seasonal (holiday) effects, or local promotions. To be sure, we should check transaction counts per month, holiday calendars, and moving averages. The findings can help plan stock and staff for busy and slow periods.(aciklamayla bunu data kisminda rapora ekle)"""

monthly = df.resample('M', on='datetime')['amount'].sum()
monthly.plot(marker='o', title='Monthly Sales', figsize=(8,4))

df['datetime'] = pd.to_datetime(df['datetime'])
print(df.groupby(df['datetime'].dt.to_period('M')).size())

daily = df.resample('D', on='datetime')['amount'].sum()
daily.rolling(7).mean().plot(title='Daily Sales (7-day MA)')

#day colomn
df.groupby('day')['amount'].sum().sort_values(ascending=False)

df.groupby('day')['amount'].mean().sort_values(ascending=False)

# Sales distribution by day
sns.barplot(x="day",y="amount", data=df, estimator="sum", ci=None)
plt.xlabel("Day")
plt.ylabel("Number of Sales")
plt.title("Day of Week Distribution")
plt.tight_layout()
plt.show()

df.groupby('day')['amount'].sum().sort_values(ascending=False).reset_index()

#Sales distribution by day

sns.countplot(data=df,x='time_of_day')
plt.xlabel("Day")
plt.ylabel("Number of Sales")
plt.title('Rush Hour by Time of Day')
plt.tight_layout()
plt.show()

df["product_name"].value_counts().plot(kind="pie",startangle=90,autopct="%1.1f%%",title="Most Bought Coffee",figsize=(6,6))

import statsmodels.api as sm
from statsmodels.formula.api import ols

model = ols("amount ~ C(day)", data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

"""#  How to interpret the p-value?

p < 0.05 → The difference is statistically significant.
This means that the sales truly differ depending on the day of the month.

p ≥ 0.05 → The difference is not statistically significant.
This means that the differences you see between, for example, day 1 and day 15 might just be due to random variation..

"""

